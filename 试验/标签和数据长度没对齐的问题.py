import torch
from transformers import BertModel, BertTokenizer

sen=torch.tensor([  101.,  2845.,  2193.,  2471.,  6835.,  5632.,  4507.,  3616.,  3828.,
         4510.,  1378.,  4638.,  3867.,  2622.,  2900.,  1139.,  8024.,  1853.,
         2209.,  6832.,  2209.,  6134.,  4850.,  1298.,  1744.,  2600.,  5320.,
         6848.,   715.,  5018.,   671.,  1726.,  1394.,  3187.,  3126.,  8024.,
         6848.,   715.,  2553.,  7557.,  6206.,  7028.,  3173.,   715.,  6121.,
         8024.,  6821.,  3221.,  3297.,  1400.,  4638.,  1104.,  2137.,  8024.,
         3187.,  3791.,  1086.,  3121.,  1359.,  8024.,  5445.,  3173.,  4638.,
         6848.,   715.,  2553.,  7557.,  1762.,  4385.,   818.,  2600.,  5320.,
         5101.,  3821.,  5650.,  5335.,  1936., 10327.,  8569.,  8939.,  2399.,
         2600.,  5320.,   818.,  3309.,  1168.,  3309.,  1184.,   715.,  6121.,
          511.,   102.])
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')

'''
可以看到
2001对应四个标签,
但是在文本编码时,
20对应一个值,
0对应一个
1对应一个

少了一个标签
'''
for x in sen:
    print(tokenizer.decode([x]))
